value randomSeed="1";
value maxThreads="33";
value trainOnDev="false";
value errorExampleExtractor="FirstTokenSpan";
array validLabels=(
"/education/field_of_study"FIXME);

evaluation accuracy=Accuracy();
evaluation accuracyBase=Accuracy(computeBaseline="true");
evaluation f.5=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="0.5");
evaluation f1=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="1");
evaluation prec=Precision(weighted="false", filterLabel="true");
evaluation recall=Recall(weighted="false", filterLabel="true");

ts_fn head=Head();
ts_fn ins1=NGramInside(n="1", noHead="true");
ts_fn ins2=NGramInside(n="2", noHead="true");
ts_fn ins3=NGramInside(n="3", noHead="true");
ts_fn ctxb1=NGramContext(n="1", type="BEFORE");
ts_fn ctxa1=NGramContext(n="1", type="AFTER");
ts_fn sent1=NGramSentence(n="1", noSpan="true");
ts_str_fn pos=PoSUniversal();
ts_str_fn strDef=String(cleanFn="CatDefaultCleanFn");
ts_str_fn strStem=String(cleanFn="CatStemCleanFn");
ts_str_fn strBoW=String(cleanFn="CatBagOfWordsFeatureCleanFn");
str_fn pre=Affix(nMin="3", nMax="3", type="PREFIX"); 
str_fn suf=Affix(nMin="3", nMax="3", type="SUFFIX"); 
str_fn filter=Filter(type="SUBSTRING", filter="", filterTransform=Identity());
str_fn filter_s=Filter(type="SUFFIX", filter="", filterTransform=Identity());
str_fn filter_p=Filter(type="PREFIX", filter="", filterTransform=Identity());

ts_fn insAll1=NGramInside(n="1");
ts_fn dep=DependencyRelation();
ts_str_fn lemma = TokenAnnotation(annotationType="lemma");
ts_str_fn posLemma = RelationStr(f1=${pos}, f2=${lemma}, relationSymbol="_");
ts_fn beforeAfterS3 = NGramContext(type="BEFORE_AND_AFTER", n="3", allowSqueeze="true");
ts_str_fn depLemma = TokenSpanPathStr(mode="ALL", pathLength="1", spanFn1=${dep}, strFn=${posLemma}); 
ts_str_fn depRel = TokenSpanPathStr(mode="ONLY_RELATIONS", pathLength="1", spanFn1=${dep}, strFn=${posLemma}); 

feature fpos=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=${pos});
feature fposb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxb1}));
feature fposa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxa1}));
feature fctxb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${ctxb1}));
feature fctxa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${ctxa1}));
feature fphrh1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${head}));
feature fphr2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${ins2}));
feature fphr3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${ins3}));
feature ftcnt=TokenCount(maxCount="5", tokenExtractor="AllTokenSpans");
feature fform=StringForm(stringExtractor="FirstTokenSpan", minFeatureOccurrence="2");
feature fpreh=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre} o ${strDef} o ${head}));
feature fsufh=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf} o ${strDef} o ${head}));
feature fpre=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre} o ${strDef} o ${ins1}));
feature fsuf=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf} o ${strDef} o ${ins1}));
feature fsent1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${lemma} o ${sent1}));

feature fner=Ner(useTypes="true", tokenExtractor="AllTokenSpans");
feature fgnp=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="FirstTokenSpan", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnps=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="SentenceNELLNounPhrases", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd1=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentUnigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd2=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentBigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd3=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentTrigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");

feature fctx3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${posLemma} o ${insAll1} o ${beforeAfterS3}));
feature fins1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${posLemma} o ${insAll1}));
feature fdep=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${depLemma}));
feature fdepRel=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${depRel}));

model lr=Areg(l1="0", l2="0", convergenceEpsilon=".00001", maxTrainingExamples="1000001", batchSize="200", evaluationIterations="200", maxEvaluationConstantIterations="500", weightedLabels="false", computeTestEvaluations="false")
{
	array validLabels=${validLabels};
};

gs g=GridSearch() {
	dimension l2=Dimension(name="l2", values=(.00000001,.0000001,.000001,.00001,.0001,.001,.01,.1,1,10), trainingDimension="true");
	dimension ct=Dimension(name="classificationThreshold", values=(.5,.6,.7,.8,.9), trainingDimension="false");
	
	model model=${lr};
	evaluation evaluation=${f1};
};
